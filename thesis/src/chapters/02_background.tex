% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Background}\label{ch:background}

\section{Datalog}\label{sec:datalog}

Datalog~\cite{green2013datalog} is a declarative logic programming language
invented in the 80s which is primarily used for expressing queries to retrieve
data in database systems.
A Datalog program (or query) consists of a set of rules, which are used to
derive new facts from existing ones.

\subsection{Syntax and Semantics}\label{sec:datalog-syntax-semantics}

Rules define \emph{predicates} and are syntactially expressed in the form of
Horn clauses, which are logical implications adhering to this structure:

\begin{equation}
	\underbrace{
	\underbrace{r}_{\text{head}}
	\text{ :- }
	\underbrace{
	\underbrace{a_1}_{\text{atom}},
	\ldots,
	\underbrace{a_n}_{\text{atom}}.
	}_{\text{body}}
	}_{\text{rule}}
\end{equation}

The left-hand side of a rule is called \emph{head} and the right-hand side \emph{body}.
A head consists of an identifier, which also defines the name of the predicate
the rule contributes to define, as well as a comma-separated list of expressions
which may reference variables defined in the rule's body.
A body is a comma-separated sequence of \emph{atoms} followed by a trailing ``.'' (dot).
An atom is either referencing another \emph{predicate} to bring some of its
variables into scope or imposing a boolean condition.
A condition can either restrict a variable's value range (e.g. \(x = 3\))
or specify a relationship with another variable (e.g. \(x = y\)).

A rule (a predicate) is said to be \emph{self-recursive}
if it references itself in its (one of its) body (bodies).
Moreover, it is allowed to have multiple rules sharing the same head, that is,
they have the same identifier and list of expressions.
In that case, they jointly define the predicate named after their heads' identifier.

\ref{code:trans-closure-datalog} illustrates the concepts for the
computation of a graph's transitive closure with Datalog.
It contains three rules which define the two predicates, ``edge'' and ``closure''.
The ``edge'' predicate has an empty body,
leaving just the dot on the rule's right-hand side.
The ``closure'' predicate is defined through the last two rules which share
the same head.
The last rule's body consists of three atoms: The first one references itself,
rendering the rule self-recursive, the second one references the ``edge'' predicate,
and the last atom defines a condition, restricting the value range of the
``active'' variable.
The first two atoms bring the variables ``from'', ``via'', ``to'', and ``active''
into scope.
The appearance of the same variable ``via'' in both atoms implies that their
values must be equal, i.e.,
\(a_1(x), a_2(x)\) is a shorthand for \(a_1(x_1), a_2(x_2), x_1 = x_2\).
Next to defining the name of the predicate, the head specifies that the ``from''
and ``to'' variables are exposed for the ``closure'' predicate.
The second rule's body consists of two atoms but does not introduce anything new
syntactically.

\begin{figure}[htpb]
	\centering
	% \begin{tabular}{c}
	\begin{lstlisting}[keepspaces]
edge(from, to, active) :- .
closure(from, to)      :- edge(from, to, active), active = true.
closure(from, to)      :- closure(from, via), edge(via, to, active),
                          active = true.\end{lstlisting}
	% \end{tabular}
	\caption{An exemplary computation of a graph's transitive closure with Datalog.}\label{code:trans-closure-datalog}
\end{figure}

Semantically, a rule can be read from right to left: The body's atoms are
connected with conjunctions and all variable assignments that satisfy that term
form a new \emph{fact} of the predicate which the rule defines.
Here, Datalog's close relationship to first-order logic becomes apparent:
Every rule is implicitly allquantified over its variables.
If \(x_1, \ldots x_m\) are the variables of the rule's body, the rule can be read as
``for all \(x_1, \ldots, x_m\) it holds that \(a_1\) and \ldots\ and \(a_n\) imply \(r\)''
and expressed\footnotemark{} mathematically as:

\footnotetext{
	Not every atom or head must reference all variables.
}

\begin{equation}
	\forall x_1, \ldots, x_m: a_1(x_1, \ldots, x_m) \land \ldots \land a_n(x_1, \ldots, x_m) \Rightarrow r(x_1, \ldots, x_m) \\
\end{equation}

If there are multiple rules with the same head, their bodies' conjunctions
are connected through a disjunction~\cite{abo2024convergence}.
\emph{Fact rules} are special rules without a body (with $n=0$), e.g.
\( r \text{ :- } .\), and they are assumed to be unconditionally true.
Unlike \emph{derived facts}, \emph{base facts} of fact rules are externally
given and not derived from other rules.
Hence, the predicates defined by fact rules and the predicates defined by regular
rules (those with a body) can be partioned into two disjoint sets:
\emph{\acfp{EDBP}} and \emph{\acfp{IDBP}}. Only \acp{IDBP} may reference \acp{EDBP}
but not vice-versa.
Facts contain \emph{fields} that can assume basic scalar types such as strings,
numbers, or booleans.
Moreover, for a regular rule to be valid the \emph{range-restriction property}
must be satisfied~\cite{green2013datalog}:
It demands that every variable occuring in the head of a rule must also occur
at least once in a predicate atom of its body.

Allowing rules to be self-recursive equips Datalog with the ability to express
repeated computations, therefore requiring a termination condition.
Datalog uses least-fixpoint semantics, under which the computation terminates
if an additional iteration of the repeated computation does not alter the result
anymore \emph{for the first time}.
This has some consequences but before discussing them I am now in position
to illustrate the semantics of the example query in \ref{code:trans-closure-datalog}.
For a more formal treatment of Datalog's semantics, I refer to \cite{green2013datalog}
which provides an overview on three equivalent formalisms to precisely define
Datalog's semantics, model-theoretic, fixpoint-theoretic, and proof-theoretic.

The ``edge'' \ac{EDBP} contains all edges of a graph and these are given
externally, e.g., through an insertion into the database.
The predicate defines the fields ``from'', ``to'', and ``active''.
The former two specify \emph{from which node to which other node} an edge points to.
The latter field can be used to enable or disable an edge but this is just
a contrived example to demonstrate a condition on a variable.
The ``closure'' \ac{IDBP} specifies the actual computation of the graph's
transitive closure.
The first rule of its definition specifies the computation's starting point,
that is, all pairs of nodes which (1) are reachable through a path length of 1
and (2) are marked as ``active''.
The second rule takes all node pairs identified so far, which are reachable
through paths of length \(n\), and discovers new node pairs connected
through a path of length \(n + 1\),
again while restricting the edges to be marked as ``active''.
The computation terminates if a (re)application of the rules on top of the
currently discovered facts does not produce more facts,
upon which the least-fixpoint is attained.

Least-fixpoint computations can only solve problems whose solutions
are monotonically growing, as otherwise the least-fixpoint solution may not
terminate at the optimal solution.
This can be thought of being stuck at a local minimum instead of finding
a global optimum in non-convex optimization.
Another issue, not necessarily tied to least-fixpoint computations but shared
with all termination criteria, is that it is impossible to prevent
non-terminating computations in general.
Consider a small variation on the computation in \ref{code:trans-closure-datalog}.
If every edge carried a non-negative weight and the query should additionally
keep track of the cumulated weight for each node pair of the transitive closure,
the computation's termination hinges on the graph being cycle-free.
With cycles, the computation would walk cycles endlessly and constantly discover
higher cumulated weights for node pairs which are part of a cycle.
A discussion of alternatives to fixpoints in the context of SQL
can be found in \cite{hirn2023fix}.

Traditionally, Datalog has set semantics and does not support multisets.
Moreover, it does not use name-based-indexing but positional-indexing.
As Datalog lacks a formal specification, there are many flavours of it,
and later I define my own dialect of it which supports multisets and
uses name-based-indexing.

Due to the use of relational algebra as an \ac{IR} in \ref{ch:results},
I point out some connections to it as well as to SQL
because the latter is the predominant frontend to relational algebra.
The equivalent of a \emph{relation} in Datalog is a predicate:
They both define a name under which \emph{tuples} (facts in Datalog; rows in SQL)
are made available. SQL calls this construct a \emph{table}.
Yet, relational algebra does not distinguish between base facts and derived facts
the same way Datalog does.
The closest counterpart to derived facts are the tuples of a materialized view
in SQL.
I suspect that this is due to the little emphasis on composition in SQL.

\subsection{Negation Extension}\label{sec:datalog-negation}

The Datalog semantics presented so far is not sufficient to express,
for instance, relational algebra's set difference operator or the advanced example
in \ref{code:mvr-crdt-datalog} from \ref{ch:intro} due to its lack of negation.
However, introducing negation without any restrictions to Datalog causes
semantic issues, as \ref{code:negated-datalog-issue} demonstrates:

\begin{figure}[htpb]
	\centering
	% \begin{tabular}{c}
	\begin{lstlisting}[keepspaces]
// TODO: Is there a better example? Maybe with some real-world ties?
r_1 :- not r_2.
r_2 :- not r_1.\end{lstlisting}
	% \end{tabular}
	\caption{A negated Datalog program with unclear semantics~\cite{green2013datalog}.}\label{code:negated-datalog-issue}
\end{figure}

TODO: Exlain the issue.

The most common resolution to reimpart unambiguous semantics to negative Datalog
is called \emph{stratified negation}~\cite{green2013datalog} which is a syntactic
restriction on the use of negation and recursion:
For a \emph{recursive} rule, it disallows negation in its body
(or in any of its predicates' bodies it references in its body).
To characterize the semantics of stratified negation,
an even stricter syntactic restriction called \emph{semipositive Datalog}
is introduced first.

Semipositive Datalog only allows the negation of \acp{EDBP} but not \acp{IDBP}
in the body of a rule defining an \ac{IDBP}.
Furthermore, it demands that every variable occuring in the body of a rule must
occur in at least one positive (non-negated) predicate atom,
which is referred to as \emph{safety condition}~\cite{green2013datalog}.
If it was not satisfied, the result of a query would not just depend on the
actual content of a database anymore, and may be infinite.
The semantics of negated \acp{EDBP} follows the intuition that
the negation of an \ac{EDBP} within the body of an \ac{IDBP} (together with
the safety condition) behaves like the set difference operator
in relational algebra.

Datalog with stratified negation relaxes the restriction and allows the
negation of \acp{IDBP} as long as the predicates are \emph{stratifiable}.
A Datalog program is stratifiable if it can be partitioned into \emph{ordered}
strata \(P_1, \ldots, P_n\) such that:

\begin{itemize}
	\item If the predicate \(p_b\) occurs in the body of a rule defining
	      the predicate \(p_h\), then \(p_b \in P_i\), \(p_h \in P_j\), and
	      \(i \leq j\).
	\item If the predicate \(p_b\) occurs in the body of a rule defining
	      the predicate \(p_h\) \emph{negatedly}, then \(p_b \in P_i\),
	      \(p_h \in P_j\), and \(i < j\)~\cite{green2013datalog}.
\end{itemize}

To compute a stratification of a negative Datalog program \(P\),
its \emph{precedence graph} \(G_P\) can be utilized~\cite{green2013datalog}.
It is a directed graph whose nodes are the predicates of \(P\).
Furthermore, there is

\begin{itemize}
	\item a \emph{positive} edge
	      (unlabeled in \ref{fig:precedence-graph-mvr-crdt})
	      from predicate \(p_i\) to predicate \(p_j\)
	      if \(p_i\) occurs in the body of a rule defining \(p_j\) as well as
	\item a \emph{negative} edge
	      (labeled with \(\lnot\) in \ref{fig:precedence-graph-mvr-crdt})
	      from \(p_i\) to \(p_j\)
	      if \(p_i\) occurs \emph{negatedly} in the body of a rule defining \(p_j\).
\end{itemize}

The precedence graph can be used to not only check whether a Datalog
program is stratifiable but also to compute a stratification of it.
The former is enabled by the theorem that a Datalog program is stratifiable
if and only if its precedence graph \(G_p\) does not contain a cycle
with a negative edge~\cite{green2013datalog}.
For the latter, two steps are required.
First, the precedence graph's strongly connected components form the
\emph{strata} of the Datalog program.
Second, to find a valid sequence of the strata,
the strongly connected components have to be topologically sorted.

The resulting order of strata is what imparts the semantics to stratifiable
Datalog programs:
Every stratum \(P_{i+1}\) is evaluated after its previous stratum \(P_{i}\)
as if it was a \emph{semipositive} Datalog program with the \acp{IDBP}
of \(P_{i+1}\) treated as if they were \acp{EDBP}.
The semantics are well-defined because next to their existence,
which are given by construction, they are also unique because any freedom
in choosing the strata and their order does not lead to different results,
as long as the underlying data is the same~\cite{apt1988towards}.

\ref{fig:precedence-graph-mvr-crdt} shows the precedence graph
of \ref{code:mvr-crdt-datalog}.
It contains two negative edges but since they are not part of a cycle,
the program is stratifiable.
The only cycle is the self-recursion of the ``isCausallyReady'' predicate,
denoted by a positive self-loop edge.
In general, if a precedence graph contained a cycle involving multiple nodes,
the program contains \emph{mutually recursive} predicates.

\input{figures/precedence_graph.tex}

\section{\acsp{CRDT} and Coordination-Free Environments}\label{sec:crdt-coordination-free}

\acfp{CRDT}\footnotemark{} try to solve the problems arising from asynchronous
collaboration over message-passing networks on the data structure level and
without the help of a central entity coordinating replicas and message flows.
In these coordination-free environments, in which replicas can be offline
for extended periods of time but are still permitted to write (and read)
to (from) their local state at any time,
the naive delivery of messages allows messages to be arbitrarily delayed,
to be reordered, and the same message may be delivered multiple times.
\acp{CRDT} address these challenges by augmenting messages (hereafter updates
to the \ac{CRDT}) with additional metadata (1) to preserve the user's intention
in the ``best'' possible manner and (2) to ensure the convergence of replicas,
even after temporary divergence.

\footnotetext{
	Although \acp{CRDT} are often referred to as \emph{Conflict-free} Replicated
	Data Types, I prefer the term \emph{convergent} here because the former term
	may be a bit misleading, as conflicts can still occur, e.g., in case of
	concurrent writes to the same ``location'' of a the data type.
	I think that \acp{CRDT} are better characterized by their property that diverging
	replicas eventually \emph{converge} to the same state, given the delivery of
	the same set of updates.
	While that state may comprise conflicts, replicas uniformly agree upon them
	(and their order).
}

Compared to distributed systems that require coordination,
this model is great for two reasons.
First, it offers excellent availability and latency for writes because
they do not have to coordinate over the network with other replicas.
Second, reads also offer excellent latency but may be stale insofar that they
do not always reflect the full global state.
As a consequence of allowing offline writes,
application specific invariants (beyond the ones guaranteed by the concrete \ac{CRDT})
may be violated on the aggregate level after convergence,
even though each write individually respected the invariants (based on their
respective replica's state at write creation time).
However, this issue sometimes even affects the ``guarantees'' provided by a \ac{CRDT}:
Their correct definition is complex and error-prone,
let alone proving their convergence, even for people familiar with distributed
systems~\cite{kleppmann2022assessing, gomes2017verifying}.

The literature defines two classes of \acp{CRDT}: state-based and operation-based
which both have different requirements for correctness.
Let \( S \) be the set of all possible states of a \ac{CRDT}.
State-based \acp{CRDT} require a merge function \( \sqcup: S \times S \to S \)
which must be commutative, associative, and idempotent.
Operation-based \acp{CRDT} require that the operation functions \( op_i: S \to S \)
are commutative and applied exactly once.
Both models must adhere to these properties under the strong eventual consistency
model which demands three properties~\cite{shapiro2011comprehensive}:

\begin{enumerate}
	\item \textbf{Eventual Delivery}: All updates are eventually delivered to
	      all replicas.
	\item \textbf{Termination}: All method executions terminate.
	\item \textbf{Convergence}: All replicas that have delivered the same set of
	      updates are in an equivalent state.
\end{enumerate}

Verifying the correctness of a \ac{CRDT} is a complex,
error-prone task~\cite{gomes2017verifying, kleppmann2022assessing},
and currently has to be done for each \ac{CRDT} individually.
If, however, the set of operations on a \ac{CRDT} \emph{is} the state,
the merge function can be defined as the set union,
for which the properties of commutativity, associativity, and idempotency hold.
The critical convergence property demanded by the strong eventual consistency
model is then also trivially satisfied because the state is by definition
the set of all (delivered) operations.
Moreover, applying any \emph{pure}\footnote{
	That is, the function is deterministic and side-effect-free.
}
function \( f: S \to T \) on the state does not impede the convergence property,
as the function is applied on all replicas in the same deterministic way.
\( T \) is an arbitrary set of all possible derived states \( f \) can map to.
This approach to \acp{CRDT} is known in the literature as
\emph{pure operation-based replicated data types}~\cite{baquero2017pure, stewen2024undo},
and is used in practice in the Automerge \ac{CRDT}~\cite{automerge}.
This work explores defining the pure function \( f \) as a Datalog query
over the state comprising the operations of a \ac{CRDT}.
As Datalog evaluation is deterministic, the convergence property is satisfied.

\input{figures/system_architecture}

I provide a system overview of the approach in \autoref{fig:system-arch}.
The \emph{database layer} maintains the views \deltaO{},
as defined by the \ac{CRDT} Datalog queries formulated on the \emph{application layer},
in response to updates from both the local \deltaI{local} and remote replicas
\deltaI{remote}.
In this presentation, the application layer is responsible for forwarding updates
to the database layer but forwarding could equally well happen exclusively
on the database layer.
As explained in \ref{sec:advanced_example}, the database layer must be capable
of atomically updating all relations of a write to safeguard the \ac{CRDT} queries
against reading inconsistent state,
i.e., queries must read from a consistent snapshot of the database.

\autoref{fig:system-arch} illustrates a peer to peer architecture.
Yet, due to the lack of hierarchy, different network topologies are also possible,
e.g., a star network topology with a central server for more efficient update gossiping.
Yet, the issue of update propagation (and update integrity) is not the focus
of this research and various approaches exist in the literature~\cite{
	auvolat2019merkle, sanjuan2020merkle, kleppmann2024bluesky,
	kleppmann2022making}.
This also precludes the question of how to efficiently send just the minimum
set of missing updates to each replica.
This work only makes the basic assumption of some network topology and protocol
which ensures that each update will eventually be delivered to all replicas.
Furthermore, replicas are assumed to be non-byzantine.

\section{\Acl{IVM}}\label{sec:ivm}

\Acl{IVM}~\cite{gupta1995maintenance} deals with the problem of maintaining
an output view derived from a set of input relations in response to changes
in these inputs.
It computes the \emph{changes} to the output view which accrued through
the changes in its inputs since the last evaluation.
This renders the consumers of the output view \emph{stateful},
unlike consumers of non-incremental queries which are stateless
and obtain the full output view upon every evaluation.
The goal of \ac{IVM} is to maintain the view \emph{efficiently}, that is,
being faster than recomputing the view from scratch upon hitting an input change.
In theory, this allows to maintain output views in near-real-time over large
inputs which are subject to frequent changes, and are prohibitively expensive to
recompute from scratch every time.
In a nutshell, the promise of \ac{IVM} is to do only work proportional to the
size of the \emph{input changes}, rather than the size of the \emph{full inputs},
for a (re)evaluation of the output view.
This draws a clear line to caching based approaches, e.g., periodically
refreshed materialized views, whose evaluation is still proportional
to the size of the full inputs.
While they may provide low read latency, the output view may be stale.
\ac{IVM} tries to offer both low read latency and fresh data.

There are multiple approaches to \ac{IVM}.
Traditionally, \ac{IVM} relies on algebraic transformations within the framework
of relational algebra~\cite{gupta1995maintenance, gupta1993maintaining, pgivm}.
For a view \(V\), its derivative \(\Delta V\) has to be found to maintain
it incrementally.
Although this approach is not used in this work, I provide a small example
for finding the derivative of a join.
Let \(R\) and \(S\) be two relations and the view of interest is their
equi join \(V := R \bowtie S\) on some field (not relevant here).
Furthermore, \(\Delta V\), \(\Delta R\) and \(\Delta S\) are the set of changes
to \(V\), \(R\) and \(S\), respectively, since their last evaluation.
The state of \(V\) at the last evaluation is denoted as \(V_0\) and the
state of \(V\) which includes the changes is denoted as \(V_1\). Then:

\begin{equation}
	\begin{aligned}
		V_1                                                                                               & = V_0 + \Delta V                                                      \\
		\Leftrightarrow (R + \Delta R) \bowtie (S + \Delta S)                                             & = R \bowtie S + \Delta V                                              \\
		\Leftrightarrow R \bowtie S + R \bowtie \Delta S + \Delta R \bowtie S + \Delta R \bowtie \Delta S & = R \bowtie S + \Delta V                                              \\
		\Leftrightarrow \Delta V                                                                          & = R \bowtie \Delta S + \Delta R \bowtie S + \Delta R \bowtie \Delta S
	\end{aligned}
\end{equation}

In the transformation from the first to the second line, the definitions
of \(V_0\) and \(V_1\) are used.
Then, the \emph{bilinear} property (with respect to addition) of the join
operator is used which allows to distribute the join over the addition.
A final transformation is applied to isolate \(\Delta V\) and the well-known
derivative of the join~\cite{idris2017dynamic} is obtained, which can also be
derived through the DBSP framework~\cite{budiu2025dbsp}.
Unfortunately, the algebraic transformation approach is not applicable
to all queries and struggles to handle more complex queries like
recursive ones~\cite{budiu2025dbsp}.

More recent approaches are \emph{differential dataflow}~\cite{mcsherry2013differential}
and the later \emph{DBSP}~\cite{budiu2022dbsp, budiu2024dbsp, budiu2025dbsp}.
Their approach is fundamentally different from the algebraic one.
At first, they leave relational algebra behind and instead define a general
framework for expressing incremental computations over streams of insertions
and deletions.
Then, they show that the expressiveness of the framework is rich enough to
represent relational algebra, including recursive queries, and by extension
SQL and Datalog.

Although DBSP is more recent than differential dataflow,
they make different trade-offs instead of one superseding the other.
While differential dataflow is more complex, it offers more
flexibility~\cite{budiu2025dbsp}:
The order of inputs can be a partial order, i.e., inputs can be delivered
out-of-order.
On the other hand, DBSP requires a total order of inputs.
Another advantage of DBSP is its modular theory:
If a new operator can be expressed as a DBSP circuit, it is incrementalizable
and DBSP's framework can be used to obtain an efficient implementation
of it~\cite{budiu2025dbsp}.
In contrast, there is no general recipe to express an arbitrary operator
as a differential dataflow operator.
This work chooses DBSP over differential dataflow for three reasons.
First, its open-source implementation~\cite{feldera} provides a rich,
(somewhat) documented API\footnotemark{} to build upon.
Second, it is said to be less complex while still providing all ingredients to
to express \acp{CRDT} for my use case.
Third, differential dataflow's advantage of being able to handle out-of-order
inputs is not relevant for operation-based \acp{CRDT}, as they are designed
to be tolerant of handling out-of-order updates among replicas.
Therefore, I turn towards DBSP specific concepts which are relevant in \ref{ch:results}.

\footnotetext{
	While the documentation is not perfect and the API surface quite involved,
	it is sufficient to start building with DBSP.
	As part of this work, I have contributed a
	\href{https://github.com/feldera/feldera/pull/3893}{pull request}
	to improve the documentation of DBSP's tutorial on recursive queries.
}

\newcommand{\zset}{\(\mathcal{Z}\)-set}
\newcommand{\zsets}{\(\mathcal{Z}\)-sets}
\newcommand{\zweight}{\(\mathcal{Z}\)-weight}

DBSP relies on the concept of \zsets{}~\cite{green2007provenance}\footnotemark{}.
With \zsets{}, each tuple of the \zset{} is tagged with an integer,
called \zweight{}, which has a different meaning in different contexts.
If the tuples represent data, the \zweight{} is non-negative and indicates
the multiplicity of the tuple.
A negative \zweight{} has no meaning.
If the tuples represent changes to data, a positive \zweight{} indicates
an insertion and a negative \zweight{} indicates a deletion of a tuple.
Hence, \zsets{} allow to represent both data and changes to data with the
same construct.

\footnotetext{
	The paper calls them \(\mathcal{K}\)-relations because it operates
	in a more general context of tagging tuples with elements from a
	commutative semiring \(\mathcal{K}\).
	As DBSP uses the integers \(\mathcal{Z}\) as an instance of the
	commutative semiring, they are named \zsets{}.
}

Roughly speaking, DBSP calls a query plan a \emph{circuit}
and it can be assembled from the available operators on \emph{streams}.
Circuits can be nested to express recursive computations, in which case
the outer circuit is called the \emph{parent circuit} and the inner circuit
is called the \emph{child circuit}.
Streams are an infinite sequence of elements from \zsets{}.
Operators used in this work are \emph{distinct},
\emph{map} (for projections), \emph{filter} (for selections),
\emph{join}, \emph{plus} and \emph{minus} (for ``set'' union and difference),
as well as the \emph{delta0} operator (required for importing streams
from a parent circuit into a child circuit).
Operators can be chained to form a circuit, which can be viewed as
a directed acyclic graph of operators similar to operator trees in relational algebra.
Its root nodes are the input streams and the leaf node represents the output
stream, emitting the resulting tuples of the query.
Each input stream provides a handle to feed in updates, which are processed
according to the circuit after calling the \emph{step} method on the root
circuit.
The output stream's handle can be used to obtain the output tuples.

There are \emph{stateless} and \emph{stateful} operators.
For instance, every \emph{linear} operator, such as a \emph{filter} or a \emph{map},
is stateless, meaning that it does not have to maintain any state between
calls to the \emph{step} method.
On the other hand, bilinear operators, such as a \emph{join}, are stateful
and have to maintain state between calls to the \emph{step} method.
This has an important implication for query optimization~\cite{budiu2025dbsp}.
Unlike non-incremental query processing, the query plan is fixed and cannot
adapt to changes in the inputs after the circuit has been assembled,
\emph{without} having to construct a new circuit according to the
reoptimized query plan and then feed in all updates again to hydrate
the state of the stateful operators.
