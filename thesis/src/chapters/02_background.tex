% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Background}\label{ch:background}

\section{Datalog}\label{sec:datalog}

Datalog~\cite{green2013datalog} is a declarative logic programming language
invented in the 1980s and is a subset of Prolog.
It is primarily used for expressing queries to retrieve data in database systems.
A Datalog program (or query) consists of a set of rules,
which are used to derive new facts from existing ones.
Unlike SQL, Datalog has not been attempted to be formally standardized,
and several dialects of it exist.
The explanations of this section focus on ``conventional'' Datalog
and in \ref{sec:datalog-frontend} I define my own dialect.

\subsection{Syntax and Semantics}\label{sec:datalog-syntax-semantics}

Rules define \emph{predicates} which contain facts (also called tuples).
Syntactically, rules are expressed in the form of Horn clauses,
which are logical implications adhering to this structure:

\begin{equation}
	\underbrace{
	\underbrace{r}_{\text{head}}
	\text{ :- }
	\underbrace{
	\underbrace{a_1}_{\text{atom}},
	\ldots,
	\underbrace{a_n}_{\text{atom}}.
	}_{\text{body}}
	}_{\text{rule}}
\end{equation}

The left-hand side of a rule is called \emph{head}, the right-hand side \emph{body},
and they are separated by a ``:-'' (colon-dash).
A head consists of an identifier, which also defines the name of the predicate
whose definition the rule contributes to, as well as a comma-separated list of expressions
which may reference variables defined in the rule's body.
A body is a comma-separated sequence of \emph{atoms} followed by a trailing ``.'' (dot).
An atom either references another \emph{predicate} (hereafter predicate atoms)
to bring some of its variables into scope or imposes a boolean condition
(hereafter condition atom).
A condition can either restrict a variable's value range (e.g. \(x = 3\))
or specify a relationship with another variable (e.g. \(x = y\)).

It is permitted to have multiple rules share the same head, that is,
they have the same identifier and arity of expressions.
In that case, they jointly define the predicate named after their heads' identifier.
A rule (a predicate) is said to be \emph{self-recursive}
if it references itself in its (one of its) body (bodies).
Similarly, several distinct rules (distinct predicates) are said to be
\emph{mutually recursive} if they reference each other in their bodies.

Semantically, a rule can be read from right to left: The body's atoms are
connected with conjunctions and all variable assignments that satisfy that term
form a new \emph{fact} of the predicate which the rule defines.
Here, Datalog's close relationship to first-order logic becomes apparent:
Every rule is implicitly allquantified over its variables.
If \(x_1, \ldots x_m\) are the variables of the rule's body, a rule can be read as
``for all \(x_1, \ldots, x_m\) it holds that \(a_1\) and \ldots\ and \(a_n\) imply \(r\)''.
Mathematically, a rule can be expressed as\footnotemark{}:

\footnotetext{
	Not every atom or head must reference all variables.
}

\begin{equation}
	\forall x_1, \ldots, x_m: a_1(x_1, \ldots, x_m) \land \ldots \land a_n(x_1, \ldots, x_m) \Rightarrow r(x_1, \ldots, x_m) \\
\end{equation}

If there are multiple rules with the same head, their bodies' conjunctions
are connected through a disjunction~\cite{abo2024convergence}.
\emph{Fact rules} are special rules without a body (with $n=0$), e.g.
\( r \text{ :- } .\), and define \acp{EDBP}, whose facts are called
\emph{base facts}, are externally given, and assumed to be unconditionally true.
Regular rules with a non-empty body (with $n>0$) define \acp{IDBP}
and their facts are called \emph{derived facts}.
Facts are an ordered list of \emph{fields} that can assume basic scalar
types such as strings, numbers, or booleans.
Due to their ordering, fields are accessed through positional indexing.
Moreover, for a regular rule to be valid the \emph{range-restriction property}
must be satisfied~\cite{green2013datalog}:
It demands that every variable occurring in the head of a rule must also occur
at least once in a predicate atom of its body, to avoid a dangling reference
to a variable.

Allowing rules to be recursive equips Datalog with the ability to express
repeated computations, therefore requiring a termination condition.
Datalog uses least-fixed point semantics, under which the computation terminates
if an additional iteration of applying a program's rules does contribute further
derived facts to its result anymore \emph{for the first time}.
For a more formal treatment of Datalog's semantics, I refer to \cite{green2013datalog}
which provides an overview on three equivalent formalisms to precisely define
Datalog's semantics: Model-theoretic, fixpoint-theoretic, and proof-theoretic.

Due to the use of relational algebra as the basis for an \ac{IR}
in \ref{ch:implementation}, I point out some similarities and differences to it,
as well as to SQL, because it is the predominant frontend to relational algebra.
The counterpart of a predicate in relational algebra is a \emph{relation}, while
the equivalent in SQL is a \emph{table}.
They all define a name under which facts (tuples in relational algebra;
rows in SQL) are made available, whose fields contain scalar values.
Datalog uses positional indexing to access fields, whereas in
relational algebra and SQL, fields are accessed by their name.
Furthermore, Datalog's predicates usually offer set semantics,
and SQL's tables are defined as \emph{multisets} (bags) which permit duplicates.
The semantics of relations in relational algebra depend on the context and
can be either set-based or multiset-based.

\begin{figure}[htpb]
	\centering
	\begin{lstlisting}[keepspaces]
edge(from, to, weight)       :- .
closure(from, to, weight, 1) :- edge(from, to, weight), from = 2.
closure(from, to, cweight + weight, hopcnt + 1)
                             :- closure(from, via, cweight, hopcnt),
                                edge(via, to, weight).\end{lstlisting}
	\caption{An exemplary computation of a graph's transitive closure with Datalog.}\label{code:trans-closure-datalog}
\end{figure}

\ref{code:trans-closure-datalog} illustrates the concepts for the
computation of a graph's transitive closure with Datalog.
It contains three rules which define the two predicates, ``edge'' and ``closure''.
Due its empty body, the ``edge'' rule defines an \ac{EDBP} whose facts
contain the fields ``from'', ``to'', and ``weight'' and define an edge
in a directed, weighted graph.
The first two fields specify \emph{from which node to which other node} an edge
points to, while the third field specifies \emph{the weight of the edge}.
Their types are not specified here but can be assumed to be (non-negative) integers.
The facts of the ``edge'' predicate are not computed by Datalog but
are assumed to be given externally, e.g., through an insertion into the database.
The ``closure'' \ac{IDBP} is defined through the last two rules sharing
the same head.
The last rule's body consists of two atoms: The first one references itself,
rendering the rule self-recursive, and the second one references the ``edge'' predicate.
Together they bring the variables ``from'', ``via'', ``cweight'', ``hopcnt'',
``to'', and ``weight'' into scope.
The appearance of the same variable ``via'' in both atoms implies that their
values must be equal, i.e.,
\(a_1(x), a_2(x)\) is a shorthand for \(a_1(x_1), a_2(x_2), x_1 = x_2\).
Next to defining the name of the predicate, the head specifies that the ``from''
and ``to'' variables are exposed as the first two fields of the ``closure'' predicate,
and that its third and fourth field are defined through their respective expressions.
The second rule's body also contains the ``edge'' predicate atom and a condition
atom which restricts the ``from'' variable to be equal to the value ``2''.
The ``closure'' \ac{IDBP} specifies the actual computation of the graph's
transitive closure.
The second rule specifies the computation's starting point,
that is, all pairs of nodes which are reachable through a path of length 1
(direct edge), whose first entry is the node with id ``2''.
The third rule takes all node pairs identified so far, which are reachable
through paths of length \(n\), and discovers new node pairs connected
through a path of length \(n + 1\),
while keeping track of the cumulative weight of the path and the number of hops.
The computation terminates if a (re)application of the rules on top of the
currently derived facts does not produce new facts,
upon which the least-fixed point is attained.

Least-fixed point computations can only solve problems whose solutions
are monotonically growing, as otherwise the least-fixed point solution may not
terminate at the optimal solution.
This can be thought of being stuck at a local minimum instead of finding
a global optimum in non-convex optimization.
Another issue, not necessarily tied to least-fixed point computations but shared
with all termination criteria, is that it is impossible to prevent
non-terminating computations in general.
Due to keeping track of the cumulated weight and the number of hops,
the computation in \ref{code:trans-closure-datalog} is only guaranteed to terminate
if the graph is cycle-free.
With cycles, the computation would walk cycles endlessly and constantly discover
higher cumulated weights, for node pairs which are part of a cycle.
A discussion of alternatives to fixed points in the context of SQL
can be found in \cite{hirn2023fix}.

\subsection{Negation Extension}\label{sec:datalog-negation}

So far I presented \emph{positive} Datalog, which does not allow negation of
atoms.
This leaves Datalog's expressiveness quite limited, as, for instance, it cannot
express relational algebra's set difference operator or the examples
from \ref{code:mvr-crdt-datalog,code:mvr-store-datalog}.
However, introducing negation without any restrictions onto Datalog causes
semantic issues, as \ref{code:negative-datalog-issue} demonstrates:

\begin{figure}[htpb]
	\centering
	\begin{lstlisting}[keepspaces]
human(name)    :- .
isLiar(name)   :- human(name), not isHonest(name).
isHonest(name) :- human(name), not isLiar(name).
\end{lstlisting}
	\caption{A Datalog program with negation but unclear semantics.}\label{code:negative-datalog-issue}
\end{figure}

Assuming that the ``human'' \ac{EDBP} only contains the fact
\(\{ \mathit{epimenides} \}\)\footnotemark{}, and trying to intuitively evaluate the
``isLiar'' \ac{IDBP}, forces the evaluation to assess the ``isHonest'' \ac{IDBP}
for \textit{epimenides}, which in turn triggers the evaluation of the
``isLiar'' \ac{IDBP} for \textit{epimenides} again, and so forth.
Hence, carelessly introducing negation in Datalog causes undefined semantics
in case of recursive rules which use negation as part of their recursive definition.
To (quite literally) break this cycle, \emph{semipositive} Datalog is introduced
first, which then enables the definition the semantics of \emph{stratified}
Datalog, both of which are syntactic restrictions on the use of negation and
recursion~\cite{green2013datalog}.
While there are other approaches to reimpart unambiguous semantics to
Datalog with negation, \emph{stratified negation} is the predominant
one~\cite{green2013datalog}.

\footnotetext{
	This is inspired by the
	\href{https://en.wikipedia.org/wiki/Liar_paradox}{liars paradox},
	whose first instance is (not quite correctly) attributed to the Cretan
	philosopher Epimenides.
}

Semipositive Datalog only allows the negation of \acp{EDBP}, but not \acp{IDBP},
as part of the body of a rule defining an \ac{IDBP}.
Furthermore, it demands that every variable occurring in the body of a rule must
occur in at least one positive (non-negative) predicate atom,
which is referred to as the \emph{safety condition}~\cite{green2013datalog}.
If it was not satisfied, the result of a query would not only depend on the
actual content of a database, and may be infinite.
The semantics of negative \acp{EDBP} follows the intuition that
the negation of an \ac{EDBP} within the body of an \ac{IDBP} (together with
the safety condition) behaves similarly to the set difference operator
in relational algebra.

Stratified Datalog relaxes the restriction a bit and allows the
negation of \acp{IDBP} within a rule, as long as the predicates
are \emph{stratifiable}.
A Datalog program is stratifiable if its predicates can be partitioned into
\emph{ordered} strata \(P_1, \ldots, P_n\) such that:

\begin{itemize}
	\item If the predicate \(p_b\) occurs positively in the body of a rule
	      defining the predicate \(p_h\), then \(p_b \in P_i\), \(p_h \in P_j\),
	      and \(i \leq j\).
	\item If the predicate \(p_b\) occurs negatively in the body of a rule
	      defining the predicate \(p_h\), then \(p_b \in P_i\),
	      \(p_h \in P_j\), and \(i < j\)~\cite{green2013datalog}.
\end{itemize}

To compute a stratification of a negative Datalog program \(P\),
its \emph{precedence graph} \(G_P\) can be utilized~\cite{green2013datalog}.
It is a directed graph whose nodes are the predicates of \(P\).
Furthermore, there is

\begin{itemize}
	\item a \emph{positive} edge
	      (unlabeled in \ref{fig:precedence-graph-mvr-crdt})
	      from predicate \(p_i\) to predicate \(p_j\)
	      if \(p_i\) occurs in the body of a rule defining \(p_j\) as well as
	\item a \emph{negative} edge
	      (labeled with \(\lnot\) in \ref{fig:precedence-graph-mvr-crdt})
	      from \(p_i\) to \(p_j\)
	      if \(p_i\) occurs \emph{negatively} in the body of a rule defining \(p_j\).
\end{itemize}

The precedence graph can be used not only to check whether a Datalog
program is stratifiable but also to compute a stratification of it.
The former is enabled by the theorem that a Datalog program is stratifiable
if and only if its precedence graph \(G_p\) does not contain a cycle
with a negative edge~\cite{green2013datalog}.
For the latter, two steps are required.
First, the precedence graph's strongly connected components form the
\emph{strata} of the Datalog program.
Second, to find a valid sequence of the strata,
the strongly connected components have to be topologically sorted.

The resulting order of strata is what imparts the semantics to stratifiable
Datalog programs:
Every stratum \(P_{i+1}\) is evaluated after its previous stratum \(P_{i}\)
as if it was a \emph{semipositive} Datalog program with the \acp{IDBP}
of \(P_{i+1}\), which are defined in a lower stratum,
treated as if they were \acp{EDBP}.
The semantics are well-defined because next to their existence,
which is given by construction, they are also unique: Any freedom
in choosing the strata and their order does not lead to different results,
as long as the underlying data is the same~\cite{apt1988towards}.

\input{figures/precedence_graph.tex}

\ref{fig:precedence-graph-mvr-crdt} shows the precedence graph
of \ref{code:mvr-crdt-datalog}.
It contains two negative edges but since they are not part of a cycle,
the program is stratifiable.
For example, one possible stratification is
\(P_0 = \{\mathit{set}, \mathit{pred}, \mathit{overwrites}, \mathit{overwritten}\}\)
and
\(P_1 = \{\mathit{isRoot}, \mathit{isLeaf}, \mathit{isCausallyReady}, \mathit{mvrStore}\}\).
The only cycle is the self-recursion of the ``isCausallyReady'' predicate,
denoted by a positive self-loop edge.
In general, if a precedence graph contains a cycle involving multiple nodes,
the program contains \emph{mutually recursive} predicates.
The Datalog dialect I define in \ref{sec:datalog-frontend} sits in between
semipositive and stratified Datalog:
While it permits the negation of \acp{IDBP} in the body of a rule just like
stratified Datalog, and it supports non-negated self-recursive rules,
it does not support mutually recursive predicates.

\section{\acsp{CRDT} and Coordination-Free Environments}\label{sec:crdt-coordination-free}

Some distributed applications are required to continue operating in the presence of
network partitions because they want to support offline use cases (high availability).
Additionally, they may want support fast local writes because they cannot afford
to wait for a network round trip to happen until the write becomes
visible (low latency).
For instance, collaborative editors may want both properties to allow users
to work regardless of their network connectivity as well as to provide a
responsive user experience, offering latency bounded by local disk access instead
of a network round trip.
The high availability and low latency properties define coordination-free
environments and set them apart from distributed systems that require coordination.
Yet, reads may be stale insofar that they do not always reflect the full global state.
As a consequence, application specific invariants may be violated on the aggregate
level after convergence, even though each write individually respected the invariants,
based on their respective replica's state at write creation time.
As long as applications can tolerate temporary violations of their invariants,
they can take advantage of the high availability and low latency properties
of coordination-free environments.

\acfp{CRDT}\footnotemark{} are one solution for implementing coordination-free
environments.
They address the challenges of coordination-free environments by augmenting
writes with additional metadata (1) to preserve the user's intention
in the ``best'' possible manner and (2) to ensure the convergence of replicas,
even after temporary divergence.
\acp{CRDT} allow replicas to be offline for extended periods of time
while still permitting them to write (and read) to (from) their local state
at any time without having to coordinate with other replicas.
When replicas come back online, they exchange their local state
and converge to a common state, which is guaranteed to be the same for all
replicas that have delivered the same set of updates.

\footnotetext{
	Although \acp{CRDT} are often referred to as \emph{Conflict-free} Replicated
	Data Types, I prefer the term \emph{convergent} here because the former term
	may be a bit misleading, as conflicts can still occur, e.g., in case of
	concurrent writes to the same ``location'' of a the data type.
	I think that \acp{CRDT} are better characterized by their property that diverging
	replicas eventually \emph{converge} to the same state, given the delivery of
	the same set of updates.
	While that state may comprise conflicts, replicas uniformly agree upon them
	(and their order).
}

The literature defines two classes of \acp{CRDT}: state-based and operation-based
which both have different requirements for correctness.
Let \( S \) be the set of all possible states of a \ac{CRDT}.
State-based \acp{CRDT} require a merge function \( \sqcup: S \times S \to S \)
which must be commutative, associative, and idempotent.
Operation-based \acp{CRDT} require that the operation functions \( op_i: S \to S \)
are commutative and applied exactly once.
Both models must adhere to these properties under the \emph{strong eventual consistency}
model which demands three properties~\cite{shapiro2011comprehensive}:

\begin{enumerate}
	\item \textbf{Eventual Delivery}: All updates are eventually delivered to
	      all replicas.
	\item \textbf{Termination}: All method executions terminate.
	\item \textbf{Convergence}: All replicas that have delivered the same set of
	      updates are in an equivalent state.
\end{enumerate}

Verifying the correctness of a \ac{CRDT} is a complex,
error-prone task~\cite{gomes2017verifying, kleppmann2022assessing},
even for people familiar with distributed systems,
and currently has to be done for each \ac{CRDT} individually.
If, however, the set of operations performed on a \ac{CRDT} \emph{is} the state,
the merge function can be defined as the set union,
for which the properties of commutativity, associativity, and idempotence hold.
The convergence property demanded by the strong eventual consistency
model is then also trivially satisfied because the state is by definition
the set of all (delivered) operations.
Moreover, applying any \emph{pure}, i.e., deterministic and side-effect-free,
function \( f: S \to T \) on the state preserves the convergence property,
as the function is applied on all replicas in the same deterministic way.
\( T \) is an arbitrary set of all possible derived states \( f \) can map to.
This approach to \acp{CRDT} is known in the literature as
\emph{pure operation-based replicated data types}~\cite{baquero2017pure, stewen2024undo},
and is used in practice in the Automerge \ac{CRDT}~\cite{automerge}.
This work explores defining the pure function \( f \) as a Datalog query
over the state comprising the operations of a \ac{CRDT}.
As Datalog evaluation is deterministic, the convergence property remains satisfied.

\input{figures/system_architecture}

\ref{fig:system-arch} provides an overview of a potential system architecture.
The \emph{database layer} maintains the views \deltaO{},
as defined by the \ac{CRDT} Datalog queries formulated on the \emph{application layer},
in response to updates from both the local \deltaI{local} and remote replicas
\deltaI{remote}.
The application layer is responsible for forwarding updates to the database layer
but forwarding could equally well happen on the database layer.
As explained in \ref{sec:advanced_example}, the database layer must be capable
of atomically updating all relations of a write to safeguard the \ac{CRDT} queries
against reading inconsistent input state.

Although \ref{fig:system-arch} illustrates a peer to peer architecture,
different network topologies are also possible.
For instance, a star network topology with a central server could be used
for more efficient update dissemination.
However, the issue of update dissemination (and update integrity) is not the focus
of this work and various approaches are discussed in the literature~\cite{
	auvolat2019merkle, sanjuan2020merkle, kleppmann2024bluesky,
	kleppmann2022making}.
I only make the basic assumption of some network and protocol
which ensures that each update will eventually be delivered to all replicas.
Furthermore, replicas are assumed to be non-byzantine.

\section{Incremental View Maintenance}\label{sec:ivm}

\Acf{IVM}~\cite{gupta1995maintenance} deals with the problem of maintaining
an output view derived from a set of input relations in response to changes
in these inputs.
It computes the \emph{changes} to the output view which accrued through
the changes in its inputs since the last evaluation.
This renders the consumers of the output view \emph{stateful},
unlike consumers of non-incremental queries which are stateless
and obtain the full output view upon every evaluation.
The goal of \ac{IVM} is to maintain the view \emph{efficiently}, that is,
faster than recomputing the view from scratch upon an input change.
In theory, this allows maintaining output views in near-real-time over large
inputs which are subject to frequent changes, which would otherwise be
prohibitively expensive to recompute from scratch every time.
The promise of \ac{IVM} is to do only work proportional to the
size of the \emph{input changes}, rather than the size of the \emph{full inputs},
for a (re)evaluation of the output view.
This draws a clear line to caching based approaches, e.g., periodically
refreshed materialized views, whose evaluation is still proportional
to the size of the full inputs.
While they may provide low read latency, the output view may be stale.
\ac{IVM} tries to offer both low read latency and fresh data.

There are multiple approaches to \ac{IVM}.
Traditionally, \ac{IVM} relies on algebraic transformations within the framework
of relational algebra~\cite{gupta1995maintenance, gupta1993maintaining, pgivm}.
For a view \(V\), its derivative \(\Delta V\) has to be found to maintain
it incrementally.
Although this approach is not used in this work, I provide a small example
for finding the derivative of a join.
Let \(R\) and \(S\) be two relations and the view of interest is their
equijoin \(V := R \bowtie S\) on some field (not relevant here).
Furthermore, \(\Delta V\), \(\Delta R\) and \(\Delta S\) are the set of changes
to \(V\), \(R\) and \(S\), respectively, since their last evaluation.
The state of \(V\) at the last evaluation is denoted as \(V_0\) and the
state of \(V\) which includes the changes is denoted as \(V_1\). Then:

\begin{equation}
	\begin{aligned}
		V_1                                                                                               & = V_0 + \Delta V                                                      \\
		\Leftrightarrow (R + \Delta R) \bowtie (S + \Delta S)                                             & = R \bowtie S + \Delta V                                              \\
		\Leftrightarrow R \bowtie S + R \bowtie \Delta S + \Delta R \bowtie S + \Delta R \bowtie \Delta S & = R \bowtie S + \Delta V                                              \\
		\Leftrightarrow \Delta V                                                                          & = R \bowtie \Delta S + \Delta R \bowtie S + \Delta R \bowtie \Delta S
	\end{aligned}
\end{equation}

In the transformation from the first to the second line, the definitions
of \(V_0\) and \(V_1\) are used.
Then, the \emph{bilinear} property (with respect to addition) of the join
operator is used which allows the join to be distributed over the addition.
A final transformation is applied to isolate \(\Delta V\) and the well-known
derivative of the join~\cite{idris2017dynamic} is obtained, which can also be
derived through the DBSP framework~\cite{budiu2025dbsp}.
Unfortunately, the algebraic transformation approach is not applicable
to all queries and struggles to handle more complex queries like
recursive ones~\cite{budiu2025dbsp}.

More recent approaches are \emph{differential dataflow}~\cite{mcsherry2013differential}
and the later \emph{DBSP}~\cite{budiu2024dbsp, budiu2025dbsp}.
Their approach is fundamentally different from the algebraic one.
At first, they leave relational algebra behind and instead define a general
framework for expressing incremental computations over streams of insertions
and deletions.
Then, they show that the expressiveness of the framework is rich enough to
represent relational algebra, including recursive queries, and by extension
SQL and Datalog.

Although DBSP is more recent than differential dataflow,
they make different trade-offs instead of one superseding the other.
The main advantage of DBSP is its modular theory:
If a new operator can be expressed as a DBSP circuit, it is incrementalizable
and DBSP's framework can be used to obtain an efficient implementation
of it~\cite{budiu2025dbsp}.
In contrast, there is no general recipe to express an arbitrary operator
as a differential dataflow operator.
This work chooses DBSP over differential dataflow for three reasons.
First, its open-source implementation~\cite{felderarepo} provides a rich,
(somewhat) documented \acs{API}\footnotemark{} to build upon.
Second, it is said to be less complex while still providing all the necessary
ingredients to to express \acp{CRDT} for my use case.
Third, differential dataflow's advantage of being able to handle out-of-order
input updates is not relevant for my approach to \acp{CRDT},
as it is based on sets and set union to integrate updates,
which is a commutative operation already.

\footnotetext{
	While the documentation is not perfect and the \acs{API} surface quite involved,
	it is sufficient to start building with DBSP.
	As part of this work, I have contributed a
	\href{https://github.com/feldera/feldera/pull/3893}{pull request}
	to improve the documentation of DBSP's tutorial on recursive queries.
}

\newcommand{\zset}{\(\mathbb{Z}\)-set}
\newcommand{\zsets}{\(\mathbb{Z}\)-sets}
\newcommand{\zweight}{\(\mathbb{Z}\)-weight}

DBSP relies on the concept of \zsets{}~\cite{green2007provenance}\footnotemark{}.
With \zsets{}, each tuple of the \zset{} is tagged with an integer,
called \zweight{}, which has a different meaning in different contexts.
If the tuples represent data, the \zweight{} is non-negative and indicates
the multiplicity of the tuple.
A negative \zweight{} has no meaning.
If the tuples represent changes to data, a positive \zweight{} indicates
an insertion and a negative \zweight{} indicates a deletion of a tuple.
Hence, \zsets{} allow representing both data and changes to data with the
same construct.

\footnotetext{
	The paper calls them \(\mathbb{K}\)-relations because it operates
	in a more general context of tagging tuples with elements from a
	commutative semiring \(\mathbb{K}\).
	As DBSP uses the integers \(\mathbb{Z}\) as an instance of the
	commutative semiring, they are named \zsets{}.
}

Roughly speaking, DBSP calls a query plan a \emph{circuit}
and it can be assembled from the available operators on \emph{streams}.
Circuits can be nested to express recursive computations, in which case
the outer circuit is called the \emph{parent circuit} and the inner circuit
is called the \emph{child circuit}.
Streams are an infinite sequence of elements from \zsets{}.
Operators used in this work are \emph{distinct},
\emph{map} (for projections), \emph{filter} (for selections),
\emph{join}, \emph{plus} and \emph{minus} (for multiset union and difference),
\emph{antijoin} (for set difference with respect to some key),
as well as the \emph{delta0} operator (required for importing streams
from a parent circuit into a child circuit).
Operators can be chained to form a circuit, which can be viewed as
a directed acyclic graph of operators similar to operator trees in relational algebra.
Its root nodes are the input streams and the leaf node represents the output
stream, emitting the resulting tuples of the query.
Each input stream provides a handle to feed in updates, which are processed
according to the circuit after calling the \emph{step} method on the root
circuit.
The output stream's handle can be used to obtain the result tuples.

There are \emph{stateless} and \emph{stateful} operators.
For instance, every \emph{linear} operator, such as a \emph{filter} or a \emph{map},
is stateless, meaning that it does not have to maintain any state between
calls to the \emph{step} method.
On the other hand, bilinear operators, such as a \emph{join}, are stateful
and have to maintain state between calls to the \emph{step} method.
This has an important implication for query optimization~\cite{budiu2025dbsp}:
Unlike with non-incremental query processing, the query plan is fixed and cannot
adapt to changes in the inputs after the circuit has been assembled,
\emph{without} having to construct a new circuit according to the
reoptimized query plan and then having to feed in all updates again
to hydrate the stateful operators' state.
