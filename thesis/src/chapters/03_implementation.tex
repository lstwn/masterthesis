% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Implementation}\label{ch:implementation}

This chapter outlines my solution to the problem motivated in \ref{ch:intro}.
A fundamental prerequisite to assessing if \acp{CRDT}-as-a-query on an
incremental query engine is a viable approach, is to have such a query
engine available.
Unfortunately, the landscape of incremental Datalog query engines is sparse
and I had to implement one myself.

At the core of my query engine is a tree-walk interpreter that executes a
query plan by delegating all relational computations to the DBSP library,
while handling all operations on scalars, such as arithmetic and boolean
expressions, itself.
The query plan is represented in an \acf{IR} which is a small programming
language that supports variables, functions, and static scopes.
Particularly, the \ac{IR} supports relational operations,
such as selections, projections, and joins, to represent a query plan.
The \ac{IR} and the interpreter are described in \ref{sec:ir}.
Furthermore, the engine has a Datalog frontend.
Due to Datalog's lack of standardization, I design a Datalog dialect which is
discussed in \ref{sec:datalog-frontend}.
Finally, the transpilation from an \ac{AST} of a Datalog program to a reasonably
efficient query plan expressed in the \ac{IR} is described
in \ref{sec:datalog-to-relational-algebra}.

In general, query engines either interpret a query plan or compile the query
plan into an executable program tailored to the specific query.
I choose to implement a query engine based on interpretation for several
reasons:

\begin{enumerate}
	\item \textbf{Less complexity.}
	      An interpreter is simpler to implement and to debug than a compiler.
	      For an explorative project, this is a significant advantage.
	\item \textbf{No compile time overhead.}
	      Rust is known for its long compile times, potentially offsetting some
	      potential performance gains of compiled query execution.
	\item \textbf{Easier integration.}
	      A library is easier to integrate into a larger system than a compiler.
	      I think this is particularly relevant for applications that do not
	      exclusively run on big servers but may be accomotated on smaller
	      edge devices, which may not afford bundling a full compiler.
\end{enumerate}

My motivation to use an \ac{IR} based on relational algebra is twofold.
The first one is query optimization possibilities.
Although not being the focus of this work, query optimization on relational
algebra has been studied for decades~\cite{selinger1979access} and this approach
opens the gate to leverage this research.
Query optimization on Datalog has also been studied but to a lesser extent.
Its results could also be applied as part of the Datalog frontend prior to the
transpilation into the \ac{IR}.
Second, an \ac{IR} provides a layer of abstraction in two ways.
It allows multiple frontends to be implemented, such as a SQL frontend,
as long as the frontend can be translated into the relational algebra \ac{IR}.
Moreover, it makes it easier to change the underlying incremental computation
framework.
If there is a way to implement the relational operators with differential
dataflow for example, the \ac{IR} can be executed with it.
This presents an interesting future work direction, to better understand
both approaches' differences and performance characteristics.
Another possibility is to additionally offer a non-incremental query engine
and let the user choose which one to use.

\section{Intermediate Representation in Relational Algebra}\label{sec:ir}

My language supports the following scalar types: string, integer, boolean, and null.
Operations on scalars match what most of today's programming languages offer.
There is support for arithmetic operations (\texttt{*, /, +, -}),
logical operations (\texttt{and, or, not}),
and comparisons (\texttt{>, >=, ==, <, <=}),
as well as groupings through parenthesis in expressions.
The \ac{IR}'s variables belong to a single static scope (also lexical scope).
Static scopes are named after their property that it is \emph{statically known},
i.e., without any program execution, to which exact variable a
variable identifier points to at any given point in the program.
Besides storing scalar values, variables can also store relations and functions.
The latter renders functions first-class citizens and opens the door to
code reuse and encapsulation, as functions store snapshots to their static
environment in which they are defined.
These functions are also known as closures.

To generalize over arbitrary relations, relations' tuples are represented as
a sequence of scalars together with a schema for accessing its fields by name.
The schema is represented as another sequence of the same length as of the tuple.
Each entry contains the name of the field and if it is active or not.
I refer to the schema of relation \(R\) as \(\mathit{sch}(R)\).
\ref{tab:ir-operators} lists the supported relational operators of the \ac{IR}.
These are enough to express a wide range of queries and support my use case.
To be able to execute queries correctly, the engine needs to keep track of
the evolution of relations' schemas, as they are influenced by the operators
of a query plan.
\ref{tab:ir-schema-operators} shows the output schema of the operators from
\ref{tab:ir-operators}.

\begin{figure}[htpb]
	\centering
	\begin{tabular}{@{}p{0.11\textwidth}p{0.19\textwidth}p{0.6\textwidth}@{}}
		\toprule
		Operator           & Notation                                                    & Description                                                                                                                                                                                                                                                                                                       \\
		\midrule
		Distinct           & \(\mathit{distinct}(R)\)                                    & Removes duplicate tuples from its input relation \(R\).                                                                                                                                                                                                                                                           \\
		Union              & \(R \cup S\)                                                & Merges its input relations \(R\) and \(S\).                                                                                                                                                                                                                                                                       \\
		Difference         & \(R \setminus S\)                                           & Removes tuples from the left input relation \(R\) which are also present in the right input relation \(S\).                                                                                                                                                                                                       \\
		Alias              & \(\rho_\mathit{name}(R)\)                                   & Renames its input relation \(R\) to \(\mathit{name}\) and allows referring to it (and its fields) by that name in subsequent operators.                                                                                                                                                                           \\
		Selection          & \(\sigma_{\mathit{pred}}(R)\)                               & Filters tuples from its input relation \(R\) based on the predicate \(\mathit{pred}\).                                                                                                                                                                                                                            \\
		Projection         & \(\pi_{[(\mathit{name},\mathit{expr})]}(R)\)                & Produces a new relation with fields defined by the list of name-expression pairs. All expressions are evaluated in the context of a tuple from its input relation \(R\).                                                                                                                                          \\
		Cartesian Product  & \(R \times S\)                                              & Combines two relations by pairing every tuple from the left input relation \(R\) with every tuple from the right input relation \(S\).                                                                                                                                                                            \\
		Equijoin           & \(R \bowtie_{[(\mathit{lexpr}, \mathit{rexpr})]} S\)        & Like the cartesian product but it only emits a pair if all left-hand-side expressions of the list of pairs (\(\mathit{lexpr}\); evaluated in the context of a tuple from \(R\)) evaluate to equal values as all right-hand-side expressions (\(\mathit{rexpr}\); evaluated in the context of a tuple from \(S\)). \\
		Antijoin           & \(R \triangleright_{[(\mathit{lexpr}, \mathit{rexpr})]} S\) & Returns all tuples from the left input relation \(R\) that do \emph{not} match any tuple from the right input relation \(S\) based on the list of expression pairs (which is evaluated as for the equi join).                                                                                                     \\
		Fixpoint Iteration & \(\mu(\mathit{acc}, \mathit{step})\)                        & Executes \(\mathit{step}\) starting from \(\mathit{acc}\) for as long as there are changes to its up-to-now computed output, i.e., it stops once the \emph{least fixpoint} is attained.                                                                                                                           \\
		\bottomrule
	\end{tabular}
	\caption{Relational operators of the \ac{IR}.}\label{tab:ir-operators}
\end{figure}

\begin{figure}[htpb]
	\centering
	\begin{tabular}{@{}l@{}}
		\toprule
		\(\mathit{sch}(\mathit{distinct}(R)) = \mathit{sch}(R)\)                                                   \\
		\(\mathit{sch}(R \cup S) = \mathit{sch}(R)\) (or \(= \mathit{sch}(S)\))                                    \\
		\(\mathit{sch}(R \setminus S) = \mathit{sch}(R)\) (or \(= \mathit{sch}(S)\))                               \\
		\(\mathit{sch}(\rho_{\mathit{name}}(R)) = \mathit{sch}(R)\)                                                \\
		\(\mathit{sch}(\sigma_{\mathit{pred}}(R)) = \mathit{sch}(R)\)                                              \\
		\(\mathit{sch}(\pi_{[(\mathit{name},\mathit{expr})]}(R)) = [name]\) (projection's list of field names)     \\
		\(\mathit{sch}(R \times S) = \mathit{sch}(R) \circ \mathit{sch}(S)\) (\(\circ\) denotes concatenation)     \\
		\(\mathit{sch}(R \bowtie_{[(\mathit{lexpr}, \mathit{rexpr})]} S) = \mathit{sch}(R) \circ \mathit{sch}(S)\) \\
		\(\mathit{sch}(R \triangleright_{[(\mathit{lexpr}, \mathit{rexpr})]} S) = \mathit{sch}(R) \)               \\
		\(\mathit{sch}(\mu(\mathit{acc}, \mathit{step})) = \mathit{sch}(\mathit{acc})\)                            \\
		\bottomrule
	\end{tabular}
	\caption{The schema of the output of the operators of the \ac{IR}.}\label{tab:ir-schema-operators}
\end{figure}

The interpreter is invoked at two different stages of a query execution.
First, it is invoked to construct a DBSP circuit, i.e., a query plan in
Database terminology, from the input \ac{IR} program, which can then be
executed by DBSP's runtime. I refer to this invocation as \emph{query build-time}.
Second, while DBSP executes the circuit, the interpreter is invoked to
evaluate expressions and to access the values of variables. For instance,
this happens whenever a selection's predicate or the expressions of a projection
are evaluated. I refer to this as \emph{query execution-time}.

\input{figures/ir_transitive_closure.tex}

\ref{code:trans-closure-ir} shows the same computation of a graph's transitive
closure from \ref{sec:datalog-syntax-semantics} but expressed in pseudocode which
resembles the real representation of the \ac{IR} but with some Rust-specific
boilerplate omitted.
On the top level, it shows the main statements of the \ac{IR} program which
are all variable assignments in this case.
The defined variables, ``edges'', ``base'', and ``closure'', can then be referenced
in subsequent statements, respectively.
The ``edges'' variable is referenced by both the ``ProjectionExpr'' and the
``FixPointIterExpr''.
The ``base'' variable is referenced by the ``FixPointIterExpr''.
Due to statements returning their value as well (as long as they produce any),
the ``closure'' variable's value is also the output of the \ac{IR} program,
resembling implicit returns known from Rust or Ruby.
The ``FixPointIterExpr'' requires to specify all relations it wants to use within
its ``step'' body because DBSP requires to import relations from parent circuits
into child circuits via its \emph{delta0} operator.
To provide this, the ``imports'' and ``accumulator'' fields are used to
specify the relations which have to be injected into the context of the fixpoint
iteration's body.
Note that the ``EquiJoinExpr'' supports including a projection step,
making a trailing ``ProjectionExpr'' of the fixpoint iteration's body obsolete.
The highlighted code in blue shows the code which is executed at query
execution-time in the context of a tuple.
The other code is executed at query build-time to construct the DBSP circuit.

Prior to the execution an \ac{IR} program, the program first undergoes a
static pass to resolve all references to variables and functions except for
tuples' variables.
Tuples' variables are not yet available at query build-time but are injected
later during query execution-time. Hence, they require dynamic resolution.
Yet, resolving regular variables and functions statically has the advantage that
the interpreter can avoid dynamic lookups, which require to walk the program's
static scope tree, at both query build and execution-time.
Moreover, the static variable resolver pass fixes a problem with closures
capturing their environment, which exists if environments are implemented with
mutable instead of immutable data structures~\cite{nystrom2021crafting}.

The ability to mark fields as active or inactive allows optimizing projections:
If a projection only wants to pick a subset of a relation's fields and does not
contain any expressions which require evaluations, the projection can be executed
by just changing the relation's schema without having to run the interpreter
on the relation's tuples during query execution-time.

\section{Datalog Frontend}\label{sec:datalog-frontend}

\begin{figure}[htpb]
	\centering
	\begin{tabular}{c}
		\begin{lstlisting}[keepspaces]
// Core Datalog grammar.
program     = rule* EOF ;
rule        = head ":-" body "." ;
head        = "distinct"? IDENTIFIER "(" field ( "," field )* ")" ;
field       = IDENTIFIER ( "=" expression )? ;
body        = ( atom ( "," atom )* )? ;
atom        = ( "not"? predicate ) | "(" expression ")" | comparison ;
predicate   = IDENTIFIER "(" variable ( "," variable )* ")" ;
variable    = IDENTIFIER ( "=" IDENTIFIER )? ;

// Scalar expressions grammar.
expression  = logical_or ;
logical_or  = logical_and ( ";" logical_and )* ;
logical_and = comparison ( "," comparison )* ;
comparison  = term ( ( "==" | "!=" | ">" | ">=" | "<" | "<=" ) term )? ;
term        = factor ( ( "+" | "-" ) factor )* ;
factor      = unary ( ( "*" | "/" ) unary )* ;
unary       = ( "-" | "!" ) unary | primary ;
primary     = literal | IDENTIFIER | "(" expression ")" ;
literal     = BOOL | UINT | IINT | STRING | NULL ;

// Primitives and literals.
BOOL        = "true" | "false" ;
UINT        = DIGIT+ ;
IINT        = ( "-" | "+" )? DIGIT+ ;
STRING      = "\""<any char except "\"">*"\"" ;
IDENTIFIER  = ALPHA ( ALPHA | DIGIT )* ;
ALPHA       = "a".."z" | "A".."Z" | "_" ;
DIGIT       = "0".."9" ;
NULL        = "null" ;\end{lstlisting}
	\end{tabular}
	\caption{The grammar of my Datalog Variant.}\label{code:datalog-grammar}
\end{figure}

\ref{code:datalog-grammar} shows the grammar of my Datalog dialect\footnotemark{},
along with the syntax for literals as well as for a small expression language
which is required to specify conditions on variables.
The dialect follows ``typical'' Datalog syntax and semantics
except for some modifications.

\footnotetext{
	The grammar does not include comment support for brevity.
	The implementation supports \code{//}-EOL-style comments between rules
	and between atoms of a rule's body.
}

Traditionally, Datalog uses positional indexing to access variables from a
predicate. I decided to use name-based indexing for two reasons:
First, it aligns better with the \ac{IR} which uses relational algebra
that uses names to refer to relations' variables.
Second, positional indexing is not really a viable option in practice because
predicates (or relations) with many columns occur in real-world
database schemas, rendering positional indexing cumbersome to use.
Moreover, variables starting with an underscore are ignored but can be used
to make things more explicit.

The grammar also allows the ``distinct'' keyword to be prepended to the name
of a rule's head.
This causes the facts of the rule to be distinct, i.e., it removes duplicates
from the rule's output.
As Datalog uses set semantics traditionally, an explicit distinct operator
is usually not necessary because it is implicitly given.
Nevertheless, I deviate to allow for bag semantics for two reasons:
First, in practice it may be useful to be able to work with duplicates.
Second, enforcing set semantics can be costly performance-wise,
as it requires maintaining an index to check for duplicates.
This is similar to the situation with relational algebra and SQL.
In most formal settings, relational algebra uses set semantics but nearly
every SQL implementation in practice uses bag semantics.

Besides that, new fields can be defined through expressions in the list of fields
of a rule's head.
Yet, due to name-based indexing, a name must be provided.
The expressions can reference all variables which are in scope of the rule's body.
My dialect prohibits mutual recursion and only permits self-recursion, i.e.,
any program's precedence graph must be acyclic except for self-loops.
This is stricter than stratified Datalog which allows cycles,
provided that these cycles do not contain a negative edge.
This is a deliberate design choice to keep the implementation complexity manageable
while still being sufficient for my \ac{CRDT} use case.
\ref{sec:crdts-as-queries} shows the familiar key-value stores from \ref{ch:intro}
as well as a list \ac{CRDT} in my Datalog dialect.

The parser itself is implemented with the help of the ``nom'' library~\cite{nom}
which is a parser combinator framework for Rust.
Parser combinators are higher-order functions used to compose a parser
from smaller parsers, which makes writing a parser for an existing grammar
relatively straightforward, as long as unit tests are written for the input
parsers first. Otherwise, the origin of a parsing bug may be hard to identify.

\section{Translating Datalog to Relational Algebra}\label{sec:datalog-to-relational-algebra}

Due its declarative nature, Datalog does not specify how to execute a query
and, in particular, leaves the problem of finding a valid execution order
to the query engine.
Therefore, a precedence graph is constructed as described in \ref{sec:datalog-negation}
from the \emph{aggregated} rules of a Datalog program.
Aggregated means that all rules with the same head are combined into an
aggregated rule which contains all bodies of the original rules.
This has the advantage that every predicate is now represented by a single
aggregated rule.
Then, a topological sort is computed on top of the precedence graph using
Kahn's algorithm~\cite{kahn1962topological}.
In addition to yielding a valid execution order that respects which predicate has
to be computed before which other predicate, the algorithm also aborts in
case of a cycle in the precedence graph, thereby detecting programs that are
invalid due to use of mutual recursion.
To avoid a false positive in case of a self-recursive predicate,
the construction of the precedence graph omits self-loops.

Having found a valid execution order of the predicates,
the aggregated rules are translated into the relational algebra \ac{IR}.
The topologically sorted predicates are mapped into a sequence of statements
which assign the output of an aggregated rule to a variable with the same name.
Then, subsequent rules which depend on the output of a previous rule can refer
to the variable of a previous rule if necessary.
The last predicate in the topological sort order implicitly becomes the output
of the \ac{IR} program because the \ac{IR} supports implicit returns as well as
statements (potentially) producing values.
Due to the nature of topological sorting, the last predicate is the
predicate with the most dependencies and implicitly assumed to be the
\emph{main predicate of interest} of the Datalog program.
In case of a tie between multiple predicates, the discovery order of Kahn's
algorithm is used to break the tie.
This is a limitation in the implementation, as it does not support multiple
main predicates.
Yet, these may be useful in contexts where computations share dependencies
for performance or code maintainability, or where the results of multiple
predicates are needed at the same time.

\subsection{Translation of Aggregated Rules to the \ac{IR}}

This leaves the question of how to translate an individual aggregated rule
(predicate) into its \ac{IR} representation.
The representation is similar to an operator tree in relational algebra.
To illustrate the translation, I first discuss a \emph{naive} translation
which emerges from Datalog's reliance on first-order logic that can be
translated to set-theoretic operations.
Then, I introduce a more efficient translation which is I have implemented
for the query engine, called \emph{subpar} translation.
At first, the discussion precludes negative atoms and is limited to
non-recursive predicates, both of which I introduce successively.

\textbf{Rules with only positive atoms}.
Non-recursive rules without negative atoms can be naively translated
into a cartesian product of all predicates in the rule's body followed by
a selection.
To handle potential name collisions among the fields,
each positive predicate atom is aliased with a unique prefix.
Moreover, each referenced predicate is projected to only include the listed
fields and to potentially rename them.
The selection's predicate consists of \emph{conjunctions}
encompassing all comparison atoms in the rule's body as well as equality comparisons.
For each variable occuring in \(n > 1\) relations, \(n - 1\) equality comparisons
are included in the selection's predicate.
Each equality comparison connects a pair of relations such that there exists
a path connecting all \(n\) relations.
Each comparison has the name of the variable on the both the left-hand and right-hand
side of the comparison but prefixed with the relations' aliases, respectively.
Finally, the list of expressions of the rule's head are projected from the
selection's output.

\ref{fig:naive-plan} shows the naive query plan for the ``mvrStore'' predicate
(depicted in \ref{code:mvr-store-rule})
from the \ac{MVR} key-value \ac{CRDT} store from \ref{code:mvr-crdt-datalog}.
After combining the three positive atoms ``set'', ``isCausallyReady'', and
``isLeaf'' through a cartesian product, the selection filters out all tuples that
do not match with the variable reuse stated in the rule's body.
As the last step, the projection confines the output to the ``Key'' and ``Value''
fields given by the rule's head.

The \emph{subpar} query translation improves upon the naive translation
by avoiding the cartesian product where possible and instead folding the positive
atoms into an operator tree of equijoins.
This avoids large intermediate results of the cartesian products through
exploiting the selectivity of the join conditions.
Additionally, the subpar translation eliminates unnecessary projections
in two cases:
If the input relations are not renamed, their projections can be omitted.
Furthermore, any projection that is immediately preceded by a join can be
coalesced into the join operator, as the join operator of the \ac{IR}
allows specifying a projection to apply onto its emitted tuples.
The engine does not, however, optimize join ordering or apply other
optimization techniques, such as predicate pushdown or expression simplification,
which is why I call it \emph{subpar}.

\ref{fig:optimized-plan} shows the subpar query plan for the ``mvrStore''
predicate.
It eliminates all cartesian products and replaces them with equijoins.
The projections and aliases can also be omitted, reducing the operator count
from ten to two,
while also avoiding cartesian products' large intermediate results.

\input{figures/naive_vs_optimized_query_plan.tex}

\textbf{Rules with positive and negative atoms}.
To handle rules with both positive and negative atoms,
all atoms in the rule's body are partioned into positive and negative atoms.
First, the positive atoms are translated into an operator tree as described above.
Then, the negative atoms are folded into an operator tree of antijoins,
starting from the positive atoms' operator tree.
The partitioning ensures that the positive atoms are evaluated first.
Due to the \emph{safety condition} of negative Datalog (\ref{sec:datalog-negation}),
it is guaranteed that all variables referenced in the negative atoms occur
at least once in the positive atoms, too, rendering the antijoins well-defined.
For the negative atoms, the antijoin operator is useful, as it does not
require schema equality between its input relations, unlike the set difference operator.
In theory, the set difference operator could also be used but it may result
in projections cutting off fields to ensure schema equality between its inputs.
Yet, in some cases, the previously cut off fields have to be joined back in later.
To avoid this, I use the antijoin operator, which allows a set difference
to be specified with respect to a list of expressions.

\ref{fig:negative-query-plan} shows a query plan for a variant of the
``mvrStore'' predicate which inlines the ``isLeaf'' predicate
(see \ref{code:mvr-store-rule-inlined}).
This causes the ``mvrStore'' rule to contain the negative ``overwritten''
predicate atom in its body.
The colored rectangles highlight the partitioning of the atoms into positive
and negative parts with their respective translations.
The query plan for the positive atoms is the input to the antijoin operation
to cover the negative ``overwritten'' atom.
The final projection to the ``Key'' and ``Value'' fields remains the same as
before.
Yet, unlike the equijoin, the antijoin does not support including a projection
step.

In case of multiple rules with the same head,
every rule's translated operator tree is folded into a union.
The use of the union is unproblematic here, as the equal heads of the rules
ensure that the outputs of the operator trees share the same schema.
If a rule's head is marked as ``distinct'', its output (after the projection
to the heads' variables) is wrapped in a distinct operator to eliminate
duplicates.
Should \emph{all} rules, which share the same head, be marked as ``distinct'',
the union is performed first and only a single distinct is applied to the union's
output, as opposed to applying a distinct operator to each rule's output and
then performing the union, which would not only be less efficient but also
semantically incorrect:
If multiple rules produce the same output tuple, the union operator would
not eliminate duplicates, as it is not a set operator but a multiset operator.
Moreover, each rule with an empty body (defining an \ac{EDBP})
is translated into a relation literal of the \ac{IR},
specifying the relation's name and its field names which can be accessed.

\textbf{Self-recursive rules}.
Self-recursive predicates are translated into a fixpoint iteration of the \ac{IR},
as shown in \ref{code:trans-closure-ir}.
To do so, the fixpoint iteration's ``imports'', ``accumulator'', and ``step''
(iteration body) have to be defined.
They require to partition the aggregated rule's bodies into \emph{non-recursive}
and \emph{recursive} bodies, which are individually translated into operator trees
as described above.
The accumulator is the initial value of the fixpoint iteration and given by
the union of the \emph{non-recursive} bodies' operator trees of the aggregated rule.
The imports are all predicates that are referenced in the rule's \emph{recursive}
bodies, except for the recursive predicate itself, which does not require importing.
The iteration body is the union of the \emph{recursive} bodies' operator trees
of the aggregated rule.
In case any of the unions only contains a single input, the union is omitted
and the sole input is used instead.

\input{figures/negative_query_plan.tex}
