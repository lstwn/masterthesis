% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Results}\label{ch:results}

This chapter outlines my solution to the problem motivated in \ref{ch:intro}.
Next to describing the implementation, I also discuss design choices
and the reasoning behind them.
\ref{sec:incremental-query-engine} deals with the incremental query engine,
which is the core of the solution.
\ref{sec:crdts-as-queries} shows another \ac{CRDT} expressed with Datalog.

\section{Incremental Query Engine}\label{sec:incremental-query-engine}

Query execution refers to the process of executing a query plan to retrieve
the results of a query.
In general, there are two predominant approaches to query execution,
\emph{interpreted} and \emph{compiled} query execution.
The former executes a query by interpreting its query plan at run-time,
usually on the granularity level of an operator of the query plan.
It exists in three main variants: Tuple-at-a-time (``volcano model''),
column-at-a-time and vector-at-a-time execution,
of which the last one is the most efficient~\cite{zukowski2005monetdb}.
Compiled query execution has been pioneered by the HyPer main memory database
system~\cite{neumann2011efficiently}.
It compiles a query plan into an executable tailored to the specific query,
can therefore take advantage of, e.g., combining multiple non-blocking operators
into a single loop, and avoids interpretation overhead.
While compiled query execution sounds more promising in terms of performance,
it is more complex to implement, debug, and, surprisingly, its performance
is not strictly better~\cite{kersten2018everything}.

Feldera, the company behind the commercial offering building upon the
open-source DBSP library, offers a SQL-to-DBSP compiler which emits
a Rust executable to execute a specific query~\cite{feldera}.
The now-abandoned Differential Datalog project~\cite{ddlog}, which relies on
differential dataflow, also compiles a query plan into a Rust executable.
Yet, in this work, I chose to implement an interpreter. This has several reasons:

\begin{enumerate}
	\item \textbf{Less complexity.}
	      An interpreter is simpler to implement and to debug than a compiler.
	      For an explorative project, this is a significant advantage.
	\item \textbf{No compile time overhead.}
	      Rust is known for its long compile times, potentially offsetting some
	      of the performance gains of compiled query execution.
	\item \textbf{Easier integration.}
	      A library is easier to integrate into a larger system than a compiler.
	      I think this is particularly relevant for applications that do not
	      exclusively run on big servers but may be accomotated on smaller
	      edge devices, which may not afford bundling a full compiler.
\end{enumerate}

The incremental query engine works as follows.
At its core is a tree-walk interpreter that executes a query plan by delegating
all relational computations to the DBSP library but handling all operations
on scalars, such as arithmetic and boolean expressions, itself.
The query plan is represented in an \acf{IR} which is a small programming
language that supports variables, functions, and static scopes.
Particularly, the \ac{IR} supports relational operations,
such as selections, projections, and joins, to represent a query plan.
The \ac{IR} and the interpreter is described in detail in \ref{sec:ir}.
Furthermore, the engine has a Datalog frontend.
Due to Datalog's lack of standardization, I design a Datalog dialect which is
discussed in \ref{sec:datalog-frontend}.
Finally, the translation from a Datalog \ac{AST} to a reasonably efficient
query plan expressed in the \ac{IR} is described
in \ref{sec:datalog-to-relational-algebra}.

My motivation to use an \ac{IR} based on relational algebra is twofold.
The first one is query optimization possibilities.
Although not being the focus of this work, query optimization on relational
algebra has been studied for decades~\cite{selinger1979access} and this approach
opens the gate to leverage this research.
Query optimization on Datalog has also been studied but to a lesser extent.
Its results could also be applied as part of the Datalog frontend prior to the
translation into the \ac{IR}.
Second, an \ac{IR} provides a layer of abstraction in two ways.
It allows to implement multiple frontends, such as a SQL frontend, as long as
the frontend can be translated into the relational algebra \ac{IR}.
Moreover, it eases changing the underlying incremental computation framework.
If there is a way to implement the relational operators with differential
dataflow, the \ac{IR} can be executed with it.
This presents an interesting future work direction, to better understand
both approaches' differences and performance characteristics.
Another possibility is to additionally offer a non-incremental query engine
and let the user choose which one to use.

\subsection{Intermediate Representation Relational Algebra}\label{sec:ir}

The language supports the following scalar types: string, integer, boolean, and null.
Operations on scalars match what most of today's programming languages offer.
There is support for arithmetic operations (\texttt{*, /, +, -}),
logical operations (\texttt{and, or, not}),
and comparisons (\texttt{>, >=, ==, <, <=}),
as well as groupings through parenthesis in expressions.
The \ac{IR}'s variables belong to a single static scope (also lexical scope).
Static scopes are named after their property that it is \emph{statically known},
i.e., without any program execution, to which exact variable a
variable identifier points to at any given point in the program.
Besides storing scalar values, variables can also store relations and functions.
The latter renders functions first-class citizens and opens the door to
code reuse and encapsulation, as functions store snapshots to their static
environment in which they are defined.
These functions are also known as closures.

To generalize over arbitrary relations, relations' tuples are represented as
a sequence of scalars together with a schema for accessing its fields by name.
The schema is represented as another sequence of the same length as the tuple.
Each entry contains the name of the field and if it is active or not.
The latter allows for an optimization which I explain later.
\ref{tab:ir-operators} lists the supported relational operators of the \ac{IR}.
These are enough to express a wide range of queries and support my use case.

\begin{figure}[htpb]
	\centering
	\begin{tabular}{@{}p{0.11\textwidth}p{0.19\textwidth}p{0.6\textwidth}@{}}
		\toprule
		Operator           & Notation                                                    & Description                                                                                                                                                                                                                                                                                                       \\
		\midrule
		Distinct           & \(\mathit{distinct}(R)\)                                    & Removes duplicate tuples from its input relation \(R\).                                                                                                                                                                                                                                                           \\
		Union              & \(R \cup S\)                                                & Merges its input relations \(R\) and \(S\).                                                                                                                                                                                                                                                                       \\
		Difference         & \(R \setminus S\)                                           & Removes tuples from the first input relation \(R\) which are also present in the second input relation \(S\).                                                                                                                                                                                                     \\
		Selection          & \(\sigma_{\mathit{pred}}(R)\)                               & Filters tuples from its input relation \(R\) based on the predicate \(\mathit{pred}\).                                                                                                                                                                                                                            \\
		Projection         & \(\pi_{[(\mathit{name},\mathit{expr})]}(R)\)                & Produces a new relation with fields defined by the list of name-expression pairs. All expressions are evaluated in the context of a tuple from its input relation \(R\).                                                                                                                                          \\
		Cartesian Product  & \(R \times S\)                                              & Combines two relations by pairing every tuple from the first input relation \(R\) with every tuple from the second input relation \(S\).                                                                                                                                                                          \\
		Equijoin           & \(R \bowtie_{[(\mathit{lexpr}, \mathit{rexpr})]} S\)        & Like the cartesian product but it only emits a pair if all left-hand-side expressions of the list of pairs (\(\mathit{lexpr}\); evaluated in the context of a tuple from \(R\)) evaluate to equal values as all right-hand-side expressions (\(\mathit{rexpr}\); evaluated in the context of a tuple from \(S\)). \\
		Antijoin           & \(R \triangleright_{[(\mathit{lexpr}, \mathit{rexpr})]} S\) & Returns all tuples from the first input relation \(R\) that do \emph{not} match any tuple from the second input relation \(S\) based on the list of expression pairs (which is evaluated as for the equi join).                                                                                                   \\
		Fixpoint Iteration & \(\mu\) TODO                                                & Executes its body as long as there are changes to its up-to-now computed output, i.e., it stops once the \emph{least fixpoint} is attained.                                                                                                                                                                       \\
		\bottomrule
	\end{tabular}
	\caption{Relational operators of the \ac{IR}.}\label{tab:ir-operators}
\end{figure}

Schema tracking.

Running example: Transitive closure.

Compile-time vs run-time.
Compile-time: Query plan build time.
Run-time: Query plan execution time.

The interpreter is invoked at different stages of the query execution.
First, it is invoked to construct a DBSP circuit, i.e., a query plan in
Database speak, which can then be executed by DBSP.
Second, while DBSP executes the circuit, the interpreter is invoked to
evaluate expressions and to access the values of variables. For instance,
this happens when a selection's predicate or a projection's expression
is evaluated.
We built an intermediate representation (IR) that supports both operations
on scalars and relations. Moreover, the IR supports a variety of programming
language constructs, such as variables, functions, and lexical scopes.

Mention optimizations:
\begin{itemize}
	\item The static variable resolver pass.
	\item The optimizations around projections.
\end{itemize}

\subsection{Datalog Frontend}\label{sec:datalog-frontend}

\ref{code:datalog-grammar} shows the grammar of my Datalog dialect\footnotemark{},
along with the syntax for literals as well as for a small expression language
which is required to specify conditions on variables.
The dialect follows ``typical'' Datalog syntax and semantics
except for some modifications.

\footnotetext{
	The grammar does not include comment support for brevity.
	The implementation supports \code{//}-EOL-style comments between rules
	and between atoms of a rule's body.
}

Traditionally, Datalog uses positional indexing to access variables from a
predicate. I decided to use name-based indexing for two reasons:
First, it aligns better with the \ac{IR} which uses relational algebra
that uses names to refer to relations' variables.
Second, positional indexing is not really a viable option in practice because
predicates (or relations) with many columns occur in real-world
database schemas, rendering positional indexing cumbersome to use.
Moreover, variables starting with an underscore are ignored but can be used
to make things more explicit.

The grammar also allows to prepend the ``distinct'' keyword to the name
of a rule's head.
This causes the facts of the rule to be distinct, i.e., it removes duplicates
from the rule's output.
As Datalog uses set semantics traditionally, an explicit distinct operator
is usually not necessary because it is implicitly given.
Nevertheless, I deviate to allow for bag semantics for two reasons:
First, in practice it may be useful to be able to work with duplicates.
Second, enforcing set semantics can be costly performance-wise
as it requires maintaining an index to check for duplicates.
This is very similar to the situation with relational algebra and SQL.
In most formal settings, relational algebra uses set semantics but nearly
every SQL implementation in practice uses bag semantics.

Besides that, new fields can be defined through expressions in the list of fields
of a rule's head.
Yet, due to name-based indexing, a name must be provided.
The expressions can reference all variables which are in scope of the rule's body.

\begin{figure}[htpb]
	\centering
	\begin{tabular}{c}
		\begin{lstlisting}[keepspaces]
		// Core Datalog grammar.
		program     = rule* EOF ;
        rule        = head ":-" body "." ;
        head        = "distinct"? IDENTIFIER "(" field ( "," field )* ")" ;
        field       = IDENTIFIER ( "=" comparison )? ;
        body        = ( atom ( "," atom )* )? ;
        atom        = ( "not"? predicate ) | comparison ;
        predicate   = IDENTIFIER "(" variable ( "," variable )* ")" ;
        variable    = IDENTIFIER ( "=" IDENTIFIER )? ;

        // Scalar expressions and arithmetic grammar.
		comparison  = term ( ( "==" | "!=" | ">" | ">=" | "<" | "<=" ) term )? ;
		term        = factor ( ( "+" | "-" ) factor )* ;
		factor      = unary ( ( "*" | "/" ) unary )* ;
		unary       = ( "-" | "!" ) unary | primary ;
		primary     = literal | IDENTIFIER | "(" comparison ")" ;
		literal     = BOOL | UINT | IINT | STRING | NULL ;

		// Primitives and literals.
		BOOL        = "true" | "false" ;
		UINT        = DIGIT+ ;
		IINT        = ( "-" | "+" )? DIGIT+ ;
		STRING      = "\""<any char except "\"">*"\"" ;
		IDENTIFIER  = ALPHA ( ALPHA | DIGIT )* ;
		ALPHA       = "a".."z" | "A".."Z" | "_" ;
		DIGIT       = "0".."9" ;
		NULL        = "null" ;
        \end{lstlisting}
	\end{tabular}
	\caption{Grammar of the Datalog Variant.}\label{code:datalog-grammar}
\end{figure}

The dialect prohibits mutual recursion and only permits self-recursion, i.e.,
any program's precedence graph must be acyclic except for self-loops.
This is stricter than stratified Datalog which allows for cycles,
provided that these cycles do not contain a negative edge.
This is a deliberate design choice to keep the implementation complexity in check
while still being sufficient for my \ac{CRDT} use case.

\ref{code:mvr-crdt-datalog-dialect} shows the familiar \ac{MVR} key-value store
\ac{CRDT} from \ref{ch:intro} in the Datalog dialect.
Except for the inclusion of \acp{EDBP}, name-based indexing, and the explicit
use of the ``distinct'' operator, the query is similar to the one
in \ref{code:mvr-crdt-datalog}.
The \acp{EDBP} need to be included to make the their fields known to the
query engine.

\begin{figure}[htpb]
	\begin{lstlisting}[keepspaces]
// These are extensional database predicates (EDBPs).
pred(FromRepId, FromCtr, ToRepId, ToCtr) :- .
set(RepId, Ctr, Key, Value)              :- .

// These are intensional database predicates (IDBPs).
distinct overwritten(RepId, Ctr)
                    :- pred(RepId = FromRepId, Ctr = FromCtr, _ToRepId, _ToCtr).
distinct overwrites(RepId, Ctr)
                    :- pred(_FromRepId, _FromCtr, RepId = ToRepId, Ctr = ToCtr).
isRoot(RepId, Ctr)  :- set(RepId, Ctr, _Key, _Value),
                       not overwrites(RepId, Ctr).
isLeaf(RepId, Ctr)  :- set(RepId, Ctr, _Key, _Value),
                       not overwritten(RepId, Ctr).
isCausallyReady(RepId, Ctr)
                    :- isRoot(RepId, Ctr).
isCausallyReady(RepId, Ctr)
                    :- isCausallyReady(FromRepId = RepId, FromCtr = Ctr),
                       pred(FromRepId, FromCtr, RepId = ToRepId, Ctr = ToCtr).
mvrStore(Key, Value)
                    :- isLeaf(RepId, Ctr),
                       isCausallyReady(RepId, Ctr),
                       set(RepId, Ctr, Key, Value).\end{lstlisting}
	\caption{The \ac{MVR} key-value store in my Datalog dialect.}\label{code:mvr-crdt-datalog-dialect}
\end{figure}

The parser itself is implemented with the help of the ``nom'' library~\cite{nom}
which is a parser combinator framework for Rust.
Parser combinators are higher-order functions used to compose a parser
from smaller parsers, which makes writing a parser for an existing grammar
relatively straightforward, as long as unit tests are written for the input
parsers first. Otherwise, the origin of a parsing bug may be hard to identify.

\subsection{Translating Datalog to Relational Algebra}\label{sec:datalog-to-relational-algebra}

Due to Datalog's declarative nature, it does not specify how to execute a query
and leaves the problem of finding a valid execution order to the query engine.
Therefore, a precedence graph is constructed as described in \ref{sec:datalog-negation}
from the \emph{aggregated} rules of a Datalog program.
Aggregated means that all rules with the same head are combined into an
aggregated rule which contains all bodies of the original rules.
This has the advantage that every predicate is now represented by a single
aggregated rule and not by multiple rules anymore.
Then, a topological sort is computed on top of the precedence graph using
Kahn's algorithm~\cite{kahn1962topological}.
Next to yielding a valid execution order that respects which predicate has
to be computed before which other predicate, the algorithm also aborts in
case of a cycle in the precedence graph, thereby detecting invalid programs.
To avoid a false positive in case of a self-recursive predicate,
the construction of the precedence graph omits self-loops.

Having found a valid execution order of the predicates,
the aggregated rules are translated into the relational algebra \ac{IR}.
The topologically sorted predicates are mapped into a sequence of statements
which assign the output of an aggregated rule to a variable with the same name.
Then, subsequent rules which depend on the output of a previous rule can refer
to the variable of a previous rule if necessary.
During interpretation, the \ac{IR} implicitly returns the effect
of the last statement as if it was an expression,
resembling implicit returns in Rust or Ruby.
That way, the last predicate in the topological sort order becomes the output
of an \ac{IR} program.
Due to the nature of topological sortings, the last predicate is the
predicate with the most dependencies and implicitly assumed to be the
\emph{main predicate of interest} of the Datalog program.
In case of a tie between multiple predicates, the discovery order of Kahn's
algorithm is used to break the tie\footnotemark{}.

\footnotetext{
	I argue that queries with multiple main predicates deserve separate
	Datalog programs, as they are not related to each other beyond sharing
	some dependencies.
}

This leaves the question of how to translate an individual aggregated rule
(predicate) into its \ac{IR} representation.
The representation is similar to an operator tree in relational algebra.
To illustrate the translation, I first discuss a naive translation
which emerges from Datalog's reliance on first-order logic that can be
translated to set-theoretic operations.
At first, the discussion precludes negative atoms and is limited to
non-recursive predicates, both which I introduce later.
Finally, I discuss a more efficient translation which I have implemented for
the query engine.

The translations often require to \emph{fold} a sequence of atoms into
nested binary operators, forming an operator tree of the \ac{IR}.
Next to the sequence, a fold requires an \emph{accumulator} and a \emph{fold function}.
The accumulator is the starting value of the fold and in case of an empty
sequence, the fold returns the accumulator.
Otherwise, the fold function is called for each atom with the accumulator
and the current atom as arguments and returns the new accumulator for the next
iteration or the final result of the fold.
Hence, when I say that a sequence of atoms is
\emph{folded into an operator tree of a certain binary operator}, e.g., an equijoin,
\emph{starting from an operator tree} \(\mathit{acc}\), I mean that the initial
accumulator is \(\mathit{acc}\) and that the fold function nests the current
accumulator as the binary operator's left input and the current atom as the
binary operator's right input and returns that new binary operator,
which has a nesting level one deeper than its input accumulator.
The real translation in the Rust implementation relies on fold operations as well.

\textbf{Rules with only positive atoms}.
Non-recursive rules without negative atoms can be naively translated
into a cartesian product of all predicates in the rule's body followed by
a selection.
To handle potential name collisions among the fields,
each relation is aliased with a unique prefix.
Moreover, each referenced predicate is projected to only include the listed
fields and to potentially rename them.
The selection's predicate consists of \emph{conjunctions}
encompassing all comparison atoms in the rule's body as well as equality comparisons.
For each variable occuring in \(n > 1\) relations, \(n - 1\) equality comparisons
are included in the selection's predicate.
Each equality comparison connects a pair of relations such that there exists
a path connecting all \(n\) relations.
Each comparison has the name of the variable on the both the left-hand and right-hand
side of the comparison but prefixed with the relations' aliases, respectively.
Finally, the list of expressions of the rule's head are projected from the
selection's output.

\textbf{Rules with positive and negative atoms}.
To handle rules with both positive and negative atoms,
all atoms in the rule's body are partioned into positive and negative atoms.
First, the positive atoms are translated into an operator tree as described above.
Then, the negative atoms are folded into an operator tree of antijoins,
starting from the positive atoms' operator tree.
The partitioning ensures that the positive atoms are evaluated first.
Due to the \emph{safety condition} of negative Datalog (\ref{sec:datalog-negation}),
it is then guaranteed that all variables referenced in the negative atoms are
occuring at least once in the positive atoms, too, rendering the antijoins well-defined.
For the negative atoms, the antijoin operator is useful, as it does not
require schema equality between its input relations, unlike the set difference operator.
In theory, the set difference operator could also be used but it may result
in projections cutting off fields to ensure schema equality between its inputs.
Yet, in some cases, the previously cut off fields have to be joined back in later.
To avoid this, I use the antijoin operator which allows to specify set difference
with respect to a list of expressions.

\textbf{Self-recursive rules}.
Recursive vs non-recursive predicates.

In case of multiple rules with the same head,
every rule's translated operator tree is folded into a union.
Moreover, each rule with an empty body (defining an \ac{EDBP})
is translated into a relation literal of the \ac{IR}, specifying
the relation's name and its named fields that can be accessed.

To avoid the fatal cartesian product with its exponential blow-up,
I introduce the \emph{semi-naive} query translation.
It tries to use as many equijoins as possible and eliminates unnecessary projections.
Furthermore, because DBSP's join operator allows to include a projection the
emitted tuples,
the semi-naive query translation coalesces joins that are immediately followed
by a projection into a single join operator.

\input{figures/naive_vs_optimized_query_plan.tex}

\input{figures/negative_query_plan.tex}

% \(n \in \mathbb{N}\) number of rules with the same name with index \(i \in [n]\).
% \(m \in \mathbb{N}\) number of a single rule's atoms with index \(j \in [m]\).
% \(p \in \mathbb{N}\) number of the rule's head's variables with index \(k \in [p]\).
% \(p_m \in \mathbb{N}\) number of the \(m\)-th atom's variables with index \(k_m \in [p_m]\).

% \[
% 	output(v_1, \ldots, v_p) \leftarrow
% 	input_1(v_1_1, \ldots, v_v_p_1),
% 	input_2(v_2_1, \ldots, v_2_p_2),
% 	\ldots
% 	input_m(v_m_1, \ldots, v_m_p_m),
% 	filters.
% \]

% \[
% 	output(v_1, \ldots, v_p) \leftarrow \ldots .
% 	\ldots
% 	output(v_1, \ldots, v_p) \leftarrow \ldots .
% \]

\section{\acp{CRDT} as Queries}\label{sec:crdts-as-queries}

\subsection{Benchmarking}\label{sec:benchmarks}
